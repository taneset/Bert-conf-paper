# Bert-conf-paper
The implementation of paper submitted ACS.
# Abstract
The transformer-based models have remarkably improved the performance of many natural language
processing tasks in recent years. However, their input length is a limitation of these
models, mainly due to the computational complexity of the attention. Inspired by the human tendency
to ignore many words during a reading comprehension task, we experiment with the effect of
